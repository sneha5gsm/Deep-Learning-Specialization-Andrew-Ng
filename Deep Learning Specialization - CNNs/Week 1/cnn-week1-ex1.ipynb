{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0, 0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xe202248>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = a_slice_prev * W\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = s.sum()\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + b\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[[-6.99908945]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev - f + 2*pad)/stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2*pad)/stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i, :, :, :]                            # Select ith training example's padded activation\n",
    "#         print(a_prev_pad.shape)\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "#                     print('w shape')\n",
    "#                     print(W.shape)\n",
    "#                     print('w other shape')\n",
    "#                     print(W[:, :, :, c].shape)\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = -0.007801972093158612\n",
      "Z[3,2,1] = [ 0.10709871 -0.03102354 -0.52995452  0.98611224  0.65733641 -0.84239368\n",
      " -0.04608241  0.08802027]\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, : , : , c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice[vert_start:vert_end, horiz_start:horiz_end])\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.average(a_prev_slice[vert_start:vert_end, horiz_start:horiz_end])\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 0.86540763 1.13376944]]]\n",
      "\n",
      "\n",
      " [[[1.13162939 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                            \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C)) \n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i, :, :, :]\n",
    "        da_prev_pad = dA_prev_pad[i, :, :, :]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 0.24130207158952496\n",
      "dW_mean = 5.721264942488976\n",
      "db_mean = -1.2483155349054407\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = (x == np.max(x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start: vert_end, horiz_start: horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
